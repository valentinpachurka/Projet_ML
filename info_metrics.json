{
    "matrix_confusion": "La matrice de confusion est un outil essentiel pour évaluer les performances d'un modèle de classification. Elle permet de mesurer la capacité du modèle à distinguer les classes positives et négatives. Les éléments de la matrice sont :\n- Vrais Positifs (VP) : Prédictions correctes de la classe positive.\n- Vrais Négatifs (VN) : Prédictions correctes de la classe négative.\n- Faux Positifs (FP) : Prédictions erronées de la classe positive (erreur de type I).\n- Faux Négatifs (FN) : Prédictions erronées de la classe négative (erreur de type II).\n",

    "report_classif": "Le rapport de classification fournit des métriques de performance détaillées pour le modèle de classification. Chaque métrique a une signification spécifique :\n- La précision mesure la proportion de prédictions positives correctes parmi toutes les prédictions positives.\n- Le rappel mesure la proportion de vrais positifs correctement identifiés parmi toutes les observations réellement positives.\n- Le F1-score est une moyenne harmonique de la précision et du rappel, utile pour trouver un équilibre entre les deux.\n- Le support indique le nombre d'occurrences de chaque classe dans l'ensemble de test, aidant à comprendre la distribution des classes.\nLe rapport de classification est essentiel pour évaluer la performance d'un modèle de classification, en particulier dans les tâches multiclasse.\n",

    "roc_auc": "La courbe ROC (Receiver Operating Characteristic) évalue la capacité du modèle à distinguer les classes positives et négatives, couramment utilisée en classification binaire.\n- La courbe ROC trace la sensibilité (taux de vrais positifs) par rapport à la spécificité (1 - taux de faux positifs) pour différents seuils de classification.\n- Une courbe ROC qui se rapproche du coin supérieur gauche indique une meilleure performance du modèle pour discriminer les classes.\n- L'AUC (Area Under the Curve) de la courbe ROC mesure la performance globale du modèle, avec une AUC de 0,5 indiquant une performance aléatoire et 1,0 une performance parfaite.\n- La courbe ROC permet d'ajuster les seuils de classification pour trouver le meilleur compromis entre sensibilité et spécificité.\n- En résumé, une courbe ROC élevée et une grande AUC indiquent une meilleure capacité du modèle à faire des prédictions précises.\n",

    "reg_metrics": "Les métriques de régression fournissent des informations sur la performance d'un modèle de régression. Voici les métriques les plus couramment utilisées :\n- RMSE (Root Mean Squared Error) : Mesure la racine carrée de la moyenne des carrés des erreurs, indiquant l'écart moyen entre les valeurs prédites et les valeurs réelles.\n- R² (Coefficient de détermination) : Mesure la proportion de la variance totale de la variable dépendante expliquée par le modèle. Une valeur de 1 indique un ajustement parfait.\n- MAE (Mean Absolute Error) : Mesure la moyenne des valeurs absolues des erreurs, indiquant l'écart moyen entre les valeurs prédites et les valeurs réelles.\n",

    "regression_plot": "Le diagramme de dispersion est un outil visuel qui montre la relation entre les valeurs réelles et les valeurs prédites d'un modèle de régression. Il est utile pour évaluer la qualité des prédictions.\n",

    "regression_coefficient_hist": "L'histogramme des coefficients de régression est un graphique qui montre la distribution des coefficients attribués à chaque variable explicative dans un modèle de régression linéaire. Il permet de comprendre l'importance relative de chaque variable dans la prédiction.\n",

    "courbe_apprentissage": "La courbe d'apprentissage est un graphique montrant l'évolution de la performance d'un modèle en fonction de la taille de l'ensemble d'entraînement. Elle permet de détecter le sous-apprentissage et le sur-apprentissage, aidant ainsi à ajuster la complexité du modèle pour de meilleures performances."
}
